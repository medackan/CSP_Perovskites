{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUtp1uBsfR7v"
   },
   "source": [
    "**CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('M7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1Z8bfBtfVEc",
    "outputId": "e96d7410-a9dc-43a3-deb6-93c7a8d56599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è± Run 1 of 5...\n",
      "\n",
      "üîß Tuning Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5379, F1: 0.5251, Precision: 0.5274, Recall: 0.5309, ROC AUC: 0.8539, ‚è± Time: 29.87 sec\n",
      "\n",
      "üîß Tuning Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4446, F1: 0.4299, Precision: 0.4386, Recall: 0.4401, ROC AUC: 0.7991, ‚è± Time: 0.09 sec\n",
      "\n",
      "üîß Tuning K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.8535, F1: 0.8486, Precision: 0.8559, Recall: 0.8479, ROC AUC: 0.9555, ‚è± Time: 15.20 sec\n",
      "\n",
      "üîß Tuning Decision Tree...\n",
      "Decision Tree Accuracy: 0.8244, F1: 0.8203, Precision: 0.8206, Recall: 0.8205, ROC AUC: 0.8964, ‚è± Time: 31.28 sec\n",
      "\n",
      "üîß Tuning Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8864, F1: 0.8848, Precision: 0.8873, Recall: 0.8840, ROC AUC: 0.9855, ‚è± Time: 1009.89 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8910, F1: 0.8900, Precision: 0.8922, Recall: 0.8891, ROC AUC: 0.9804, ‚è± Time: 54.94 sec\n",
      "\n",
      "üîß Tuning AdaBoost...\n",
      "AdaBoost Accuracy: 0.5943, F1: 0.5912, Precision: 0.6055, Recall: 0.5907, ROC AUC: 0.8737, ‚è± Time: 242.14 sec\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8494, F1: 0.8449, Precision: 0.8495, Recall: 0.8452, ROC AUC: 0.9769, ‚è± Time: 3145.60 sec\n",
      "\n",
      "üîß Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:48:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8132, F1: 0.8082, Precision: 0.8155, Recall: 0.8093, ROC AUC: 0.9691, ‚è± Time: 78.08 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6263\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8955, F1: 0.8936, Precision: 0.8963, Recall: 0.8930, ROC AUC: 0.9878, ‚è± Time: 132.07 sec\n",
      "\n",
      "üîß Tuning Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7861, F1: 0.7816, Precision: 0.7934, Recall: 0.7813, ROC AUC: 0.9672, ‚è± Time: 685.51 sec\n",
      "\n",
      "üîß Tuning Neural Network...\n",
      "Neural Network Accuracy: 0.8701, F1: 0.8669, Precision: 0.8700, Recall: 0.8667, ROC AUC: 0.9813, ‚è± Time: 647.25 sec\n",
      "\n",
      "‚è± Run 2 of 5...\n",
      "\n",
      "üîß Tuning Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5414, F1: 0.5303, Precision: 0.5333, Recall: 0.5355, ROC AUC: 0.8579, ‚è± Time: 5.99 sec\n",
      "\n",
      "üîß Tuning Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4410, F1: 0.4290, Precision: 0.4387, Recall: 0.4375, ROC AUC: 0.8017, ‚è± Time: 0.08 sec\n",
      "\n",
      "üîß Tuning K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.8546, F1: 0.8508, Precision: 0.8578, Recall: 0.8495, ROC AUC: 0.9566, ‚è± Time: 1.73 sec\n",
      "\n",
      "üîß Tuning Decision Tree...\n",
      "Decision Tree Accuracy: 0.8180, F1: 0.8144, Precision: 0.8148, Recall: 0.8146, ROC AUC: 0.8943, ‚è± Time: 1.82 sec\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8854, F1: 0.8841, Precision: 0.8866, Recall: 0.8832, ROC AUC: 0.9853, ‚è± Time: 57.99 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8908, F1: 0.8900, Precision: 0.8924, Recall: 0.8891, ROC AUC: 0.9818, ‚è± Time: 10.18 sec\n",
      "\n",
      "üîß Tuning AdaBoost...\n",
      "AdaBoost Accuracy: 0.5807, F1: 0.5799, Precision: 0.5922, Recall: 0.5779, ROC AUC: 0.8745, ‚è± Time: 42.70 sec\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8415, F1: 0.8374, Precision: 0.8418, Recall: 0.8377, ROC AUC: 0.9772, ‚è± Time: 359.14 sec\n",
      "\n",
      "üîß Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:21:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8074, F1: 0.8030, Precision: 0.8108, Recall: 0.8034, ROC AUC: 0.9686, ‚è± Time: 5.65 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6260\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8893, F1: 0.8874, Precision: 0.8897, Recall: 0.8869, ROC AUC: 0.9882, ‚è± Time: 11.27 sec\n",
      "\n",
      "üîß Tuning Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7762, F1: 0.7709, Precision: 0.7830, Recall: 0.7714, ROC AUC: 0.9663, ‚è± Time: 87.23 sec\n",
      "\n",
      "üîß Tuning Neural Network...\n",
      "Neural Network Accuracy: 0.8693, F1: 0.8665, Precision: 0.8707, Recall: 0.8665, ROC AUC: 0.9810, ‚è± Time: 63.77 sec\n",
      "\n",
      "‚è± Run 3 of 5...\n",
      "\n",
      "üîß Tuning Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5362, F1: 0.5237, Precision: 0.5277, Recall: 0.5292, ROC AUC: 0.8569, ‚è± Time: 7.43 sec\n",
      "\n",
      "üîß Tuning Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4427, F1: 0.4281, Precision: 0.4399, Recall: 0.4398, ROC AUC: 0.8020, ‚è± Time: 0.08 sec\n",
      "\n",
      "üîß Tuning K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.8573, F1: 0.8525, Precision: 0.8583, Recall: 0.8515, ROC AUC: 0.9571, ‚è± Time: 1.28 sec\n",
      "\n",
      "üîß Tuning Decision Tree...\n",
      "Decision Tree Accuracy: 0.8233, F1: 0.8193, Precision: 0.8192, Recall: 0.8198, ROC AUC: 0.8963, ‚è± Time: 1.30 sec\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8904, F1: 0.8884, Precision: 0.8902, Recall: 0.8879, ROC AUC: 0.9844, ‚è± Time: 58.75 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8943, F1: 0.8929, Precision: 0.8945, Recall: 0.8925, ROC AUC: 0.9803, ‚è± Time: 10.42 sec\n",
      "\n",
      "üîß Tuning AdaBoost...\n",
      "AdaBoost Accuracy: 0.5912, F1: 0.5891, Precision: 0.6049, Recall: 0.5870, ROC AUC: 0.8763, ‚è± Time: 42.59 sec\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8571, F1: 0.8525, Precision: 0.8568, Recall: 0.8533, ROC AUC: 0.9782, ‚è± Time: 359.55 sec\n",
      "\n",
      "üîß Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:32:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8118, F1: 0.8065, Precision: 0.8137, Recall: 0.8076, ROC AUC: 0.9712, ‚è± Time: 7.46 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6255\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8962, F1: 0.8939, Precision: 0.8960, Recall: 0.8937, ROC AUC: 0.9881, ‚è± Time: 11.29 sec\n",
      "\n",
      "üîß Tuning Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7818, F1: 0.7761, Precision: 0.7864, Recall: 0.7764, ROC AUC: 0.9687, ‚è± Time: 91.24 sec\n",
      "\n",
      "üîß Tuning Neural Network...\n",
      "Neural Network Accuracy: 0.8767, F1: 0.8738, Precision: 0.8772, Recall: 0.8728, ROC AUC: 0.9813, ‚è± Time: 71.59 sec\n",
      "\n",
      "‚è± Run 4 of 5...\n",
      "\n",
      "üîß Tuning Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5288, F1: 0.5148, Precision: 0.5174, Recall: 0.5214, ROC AUC: 0.8525, ‚è± Time: 8.55 sec\n",
      "\n",
      "üîß Tuning Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4398, F1: 0.4254, Precision: 0.4358, Recall: 0.4352, ROC AUC: 0.7990, ‚è± Time: 0.09 sec\n",
      "\n",
      "üîß Tuning K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.8593, F1: 0.8553, Precision: 0.8606, Recall: 0.8542, ROC AUC: 0.9573, ‚è± Time: 1.27 sec\n",
      "\n",
      "üîß Tuning Decision Tree...\n",
      "Decision Tree Accuracy: 0.8167, F1: 0.8133, Precision: 0.8133, Recall: 0.8135, ROC AUC: 0.8929, ‚è± Time: 1.25 sec\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8852, F1: 0.8832, Precision: 0.8854, Recall: 0.8825, ROC AUC: 0.9844, ‚è± Time: 59.12 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8885, F1: 0.8871, Precision: 0.8888, Recall: 0.8865, ROC AUC: 0.9801, ‚è± Time: 9.55 sec\n",
      "\n",
      "üîß Tuning AdaBoost...\n",
      "AdaBoost Accuracy: 0.5724, F1: 0.5677, Precision: 0.5827, Recall: 0.5679, ROC AUC: 0.8727, ‚è± Time: 42.94 sec\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8488, F1: 0.8443, Precision: 0.8490, Recall: 0.8446, ROC AUC: 0.9767, ‚è± Time: 359.15 sec\n",
      "\n",
      "üîß Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:43:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8167, F1: 0.8115, Precision: 0.8182, Recall: 0.8125, ROC AUC: 0.9692, ‚è± Time: 7.41 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6258\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8945, F1: 0.8926, Precision: 0.8949, Recall: 0.8923, ROC AUC: 0.9877, ‚è± Time: 10.92 sec\n",
      "\n",
      "üîß Tuning Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7782, F1: 0.7725, Precision: 0.7851, Recall: 0.7729, ROC AUC: 0.9663, ‚è± Time: 90.45 sec\n",
      "\n",
      "üîß Tuning Neural Network...\n",
      "Neural Network Accuracy: 0.8649, F1: 0.8624, Precision: 0.8642, Recall: 0.8622, ROC AUC: 0.9804, ‚è± Time: 62.47 sec\n",
      "\n",
      "‚è± Run 5 of 5...\n",
      "\n",
      "üîß Tuning Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5246, F1: 0.5124, Precision: 0.5152, Recall: 0.5179, ROC AUC: 0.8494, ‚è± Time: 8.74 sec\n",
      "\n",
      "üîß Tuning Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4328, F1: 0.4202, Precision: 0.4282, Recall: 0.4287, ROC AUC: 0.7961, ‚è± Time: 0.07 sec\n",
      "\n",
      "üîß Tuning K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.8579, F1: 0.8535, Precision: 0.8595, Recall: 0.8525, ROC AUC: 0.9584, ‚è± Time: 1.29 sec\n",
      "\n",
      "üîß Tuning Decision Tree...\n",
      "Decision Tree Accuracy: 0.8231, F1: 0.8197, Precision: 0.8202, Recall: 0.8195, ROC AUC: 0.8972, ‚è± Time: 1.25 sec\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8914, F1: 0.8897, Precision: 0.8918, Recall: 0.8888, ROC AUC: 0.9853, ‚è± Time: 59.52 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8895, F1: 0.8882, Precision: 0.8902, Recall: 0.8873, ROC AUC: 0.9797, ‚è± Time: 10.51 sec\n",
      "\n",
      "üîß Tuning AdaBoost...\n",
      "AdaBoost Accuracy: 0.5616, F1: 0.5611, Precision: 0.5841, Recall: 0.5572, ROC AUC: 0.8694, ‚è± Time: 43.34 sec\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8533, F1: 0.8500, Precision: 0.8553, Recall: 0.8493, ROC AUC: 0.9779, ‚è± Time: 358.67 sec\n",
      "\n",
      "üîß Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8097, F1: 0.8055, Precision: 0.8130, Recall: 0.8053, ROC AUC: 0.9692, ‚è± Time: 7.33 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6256\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8986, F1: 0.8966, Precision: 0.8986, Recall: 0.8958, ROC AUC: 0.9882, ‚è± Time: 11.26 sec\n",
      "\n",
      "üîß Tuning Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7729, F1: 0.7681, Precision: 0.7802, Recall: 0.7676, ROC AUC: 0.9640, ‚è± Time: 91.20 sec\n",
      "\n",
      "üîß Tuning Neural Network...\n",
      "Neural Network Accuracy: 0.8684, F1: 0.8642, Precision: 0.8688, Recall: 0.8640, ROC AUC: 0.9803, ‚è± Time: 66.67 sec\n",
      "\n",
      "‚úÖ Averaged results with timings saved to 'MASTER_V7_Averaged_Results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def tune_classifiers(X_train, y_train, X_test, y_test, use_random_search=False, cv=5, best_params_dict=None):\n",
    "    results = {}\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": (LogisticRegression(max_iter=1000), {\n",
    "            'clf__C': [0.01, 0.1, 1, 10],\n",
    "            'clf__solver': ['lbfgs', 'liblinear']\n",
    "        }),\n",
    "        \"Naive Bayes\": (GaussianNB(), {}),\n",
    "        \"K-Nearest Neighbors\": (KNeighborsClassifier(), {\n",
    "            'clf__n_neighbors': [3, 5, 7, 11],\n",
    "            'clf__weights': ['uniform', 'distance']\n",
    "        }),\n",
    "        \"Decision Tree\": (DecisionTreeClassifier(random_state=42), {\n",
    "            'clf__max_depth': [15, 25, 35],\n",
    "            'clf__min_samples_split': [2, 5, 10],\n",
    "            'clf__criterion': ['gini', 'entropy']\n",
    "        }),\n",
    "        \"Random Forest\": (RandomForestClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [500, 1000],\n",
    "            'clf__max_depth': [50, None],\n",
    "            'clf__max_features': ['sqrt', 'log2']\n",
    "        }),\n",
    "        \"Extra Trees\": (ExtraTreesClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 300],\n",
    "            'clf__max_depth': [None, 50]\n",
    "        }),\n",
    "        \"AdaBoost\": (AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1]\n",
    "        }),\n",
    "        \"Gradient Boosting\": (GradientBoostingClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.05, 0.1],\n",
    "            'clf__max_depth': [3, 4]\n",
    "        }),\n",
    "        \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.05, 0.1],\n",
    "            'clf__max_depth': [3, 4]\n",
    "        }),\n",
    "        \"LightGBM\": (LGBMClassifier(random_state=242), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.05, 0.1],\n",
    "            'clf__num_leaves': [15, 31]\n",
    "        }),\n",
    "        \"Support Vector Machine\": (SVC(probability=True, random_state=42), {\n",
    "            'clf__C': [0.1, 1],\n",
    "            'clf__gamma': ['scale', 'auto']\n",
    "        }),\n",
    "        \"Neural Network\": (MLPClassifier(max_iter=5000, random_state=42), {\n",
    "            'clf__hidden_layer_sizes': [(50, 10), (100, 50)],\n",
    "            'clf__alpha': [0.005, 0.001],\n",
    "            'clf__learning_rate': ['constant', 'adaptive']\n",
    "        }),\n",
    "    }\n",
    "\n",
    "    for name, (model, param_grid) in classifiers.items():\n",
    "        print(f\"\\nüîß Tuning {name}...\")\n",
    "        model_start = time.time()\n",
    "\n",
    "        if best_params_dict and name in best_params_dict:\n",
    "            # Reuse best params\n",
    "            model_params = {k.replace('clf__', ''): v for k, v in best_params_dict[name].items()}\n",
    "            model.set_params(**model_params)\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            best_model = pipe\n",
    "            best_params = best_params_dict[name]\n",
    "\n",
    "        else:\n",
    "            # Perform tuning\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n",
    "            if param_grid:  # only tune if params exist\n",
    "                search = (RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=10,\n",
    "                                             cv=cv, n_jobs=-1, scoring='accuracy', verbose=0)\n",
    "                          if use_random_search else\n",
    "                          GridSearchCV(pipe, param_grid=param_grid, cv=cv, n_jobs=-1, scoring='accuracy', verbose=0))\n",
    "                search.fit(X_train, y_train)\n",
    "                best_model = search.best_estimator_\n",
    "                best_params = search.best_params_\n",
    "            else:\n",
    "                pipe.fit(X_train, y_train)\n",
    "                best_model = pipe\n",
    "                best_params = {}\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_prob = best_model.predict_proba(X_test) if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        roc = roc_auc_score(y_test, y_prob, multi_class='ovr') if y_prob is not None else np.nan\n",
    "\n",
    "        model_end = time.time()\n",
    "        elapsed_model = model_end - model_start\n",
    "\n",
    "        print(f\"{name} Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, \"\n",
    "              f\"Recall: {recall:.4f}, ROC AUC: {roc:.4f}, ‚è± Time: {elapsed_model:.2f} sec\")\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': acc,\n",
    "            'F1 score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'ROC AUC': roc,\n",
    "            'best_model': best_model,\n",
    "            'best_params': best_params,\n",
    "            'time': elapsed_model\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---- Load M7 Data ----\n",
    "df=pd.read_csv('M7.csv')\n",
    "X = df.drop(columns=[\"crystal_system\"])\n",
    "y = df[\"crystal_system\"]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "runs = 5\n",
    "aggregated_results = defaultdict(list)\n",
    "best_param_store = {}\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"\\n‚è± Run {i+1} of {runs}...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=242 + i, stratify=y\n",
    "    )\n",
    "\n",
    "    if i == 0:\n",
    "        run_results = tune_classifiers(X_train, y_train, X_test, y_test, use_random_search=False, cv=3)\n",
    "        best_param_store = {model_name: result['best_params'] for model_name, result in run_results.items()}\n",
    "    else:\n",
    "        run_results = tune_classifiers(X_train, y_train, X_test, y_test, cv=3, best_params_dict=best_param_store)\n",
    "\n",
    "    for model_name, scores in run_results.items():\n",
    "        aggregated_results[model_name].append(scores)\n",
    "\n",
    "# ---- Averaging across runs ----\n",
    "summary = {}\n",
    "for model, runs_scores in aggregated_results.items():\n",
    "    summary[model] = {\n",
    "        'Avg Accuracy': round(np.mean([r['accuracy'] for r in runs_scores]), 4),\n",
    "        'Avg F1 Score': round(np.mean([r['F1 score'] for r in runs_scores]), 4),\n",
    "        'Avg Precision': round(np.mean([r['precision'] for r in runs_scores]), 4),\n",
    "        'Avg Recall': round(np.mean([r['recall'] for r in runs_scores]), 4),\n",
    "        'Avg ROC AUC': round(np.nanmean([r['ROC AUC'] for r in runs_scores]), 4),\n",
    "        'Avg Time (sec)': round(np.mean([r['time'] for r in runs_scores]), 2),\n",
    "        'Best Params (from first run)': best_param_store[model]\n",
    "    }\n",
    "\n",
    "summary_df = pd.DataFrame(summary).T.reset_index().rename(columns={'index': 'Model'})\n",
    "summary_df.to_csv(\"Master_V7_Averaged_Results.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Averaged results with timings saved to 'MASTER_V7_Averaged_Results.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du6y5DvpIzPo"
   },
   "source": [
    "**SHAP ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7VrAq1Lw1sv",
    "outputId": "8f5ad8e7-a18c-494b-cddf-96057a840ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ RF üîπ\n",
      "Accuracy: 0.89\n",
      "Generating SHAP values...\n",
      "üî∏ Saved: shap_outputs/RF_shap_importance.csv\n",
      "‚è± Time taken: 1592.70 seconds\n",
      "\n",
      "üîπ ET üîπ\n",
      "Accuracy: 0.89\n",
      "Generating SHAP values...\n",
      "üî∏ Saved: shap_outputs/ET_shap_importance.csv\n",
      "‚è± Time taken: 4112.93 seconds\n",
      "\n",
      "üîπ LGBM üîπ\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6254\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.074334\n",
      "[LightGBM] [Info] Start training from score -1.891258\n",
      "[LightGBM] [Info] Start training from score -1.902982\n",
      "[LightGBM] [Info] Start training from score -2.037951\n",
      "[LightGBM] [Info] Start training from score -1.901249\n",
      "[LightGBM] [Info] Start training from score -1.895380\n",
      "[LightGBM] [Info] Start training from score -1.935055\n",
      "Accuracy: 0.89\n",
      "Generating SHAP values...\n",
      "üî∏ Saved: shap_outputs/LGBM_shap_importance.csv\n",
      "‚è± Time taken: 44.92 seconds\n",
      "\n",
      "‚úÖ Combined SHAP importances saved: shap_outputs/Combined_SHAP_importance_M7.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv('M7.csv')\n",
    "target_column = 'crystal_system'\n",
    "\n",
    "# Encode categorical features except target\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col != target_column:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# Store original class names\n",
    "class_names = list(np.unique(y))\n",
    "\n",
    "# Encode target if object\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Models dictionary in order: RF, ET, LGBM\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
    "    \"LGBM\": lgb.LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"shap_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Container for combined SHAP importances\n",
    "combined_shap_df = pd.DataFrame({\"Feature\": X_test.columns})\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüîπ {model_name} üîπ\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "\n",
    "    # SHAP analysis\n",
    "    print(\"Generating SHAP values...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # Handle SHAP output shapes\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_array = np.mean([np.abs(sv) for sv in shap_values], axis=0)\n",
    "    elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "        shap_array = np.mean(np.abs(shap_values), axis=2)\n",
    "    else:\n",
    "        shap_array = np.abs(shap_values)\n",
    "\n",
    "    # Mean importance per feature\n",
    "    mean_importance = np.mean(shap_array, axis=0)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        \"Feature\": X_test.columns,\n",
    "        \"Importance\": mean_importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Save CSV per model\n",
    "    csv_path = os.path.join(output_dir, f\"{model_name}_shap_importance.csv\")\n",
    "    feature_importance_df.to_csv(csv_path, index=False, float_format=\"%.6f\")\n",
    "    print(f\"üî∏ Saved: {csv_path}\")\n",
    "\n",
    "    # Top 10 features for SHAP summary plot\n",
    "    top10_features = feature_importance_df.head(10)[\"Feature\"].tolist()\n",
    "    X_test_top10 = X_test[top10_features]\n",
    "\n",
    "    # Adjust SHAP values for top10\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values_top10 = [sv[:, [X_test.columns.get_loc(f) for f in top10_features]] for sv in shap_values]\n",
    "    else:\n",
    "        shap_values_top10 = shap_values[:, [X_test.columns.get_loc(f) for f in top10_features]]\n",
    "\n",
    "    # Plot SHAP summary bar\n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_values_top10,\n",
    "        X_test_top10,\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    plt.title(f\"{model_name}\")\n",
    "    plt.gca().set_xlabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_dir, f\"{model_name}_shap_top10_plot.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\", dpi=400)\n",
    "    plt.close()\n",
    "\n",
    "    # Add importance to combined dataframe\n",
    "    combined_shap_df[model_name] = mean_importance\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚è± Time taken: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Save combined SHAP importances CSV\n",
    "combined_csv_path = os.path.join(output_dir, \"Combined_SHAP_importance_M7.csv\")\n",
    "combined_shap_df.to_csv(combined_csv_path, index=False, float_format=\"%.6f\")\n",
    "print(f\"\\n‚úÖ Combined SHAP importances saved: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "TP0slSSmyBD8",
    "outputId": "53ebedab-ec79-4418-934a-03481973e313"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2395812958.py:19: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('Combined_SHAP_importance_M7.csv')\n",
    "\n",
    "# Adjust LightGBM values\n",
    "df[\"LGBM\"] = df[\"LGBM\"] / 10\n",
    "\n",
    "# Recalculate Mean Importance\n",
    "df[\"Mean Importance\"] = df[[\"RandomForest\", \"ExtraTrees\", \"LGBM\"]].mean(axis=1)\n",
    "\n",
    "# Sort by Mean Importance and take top 10\n",
    "top10 = df.sort_values(\"Mean Importance\", ascending=False).head(10)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=top10,\n",
    "    y=\"Feature\",\n",
    "    x=\"Mean Importance\",\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top10_features_M7.png\", dpi=400)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdwoSaVsk4RT"
   },
   "source": [
    "**Classification with reduced features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E9noXQJW0IKk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('M7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QH-P8p1Nq81q",
    "outputId": "d1649e3c-2430-4c4a-ad5f-a6a7ce8818da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['END_B', 'MPR_A', 'AM_A', 'END_A', 'IE_B', 'BP_A', 'ARR_B', 'VR_B',\n",
       "       'ARR_A', 't', 'crystal_system'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Features to keep\n",
    "features = [\"END_B\", \"MPR_A\", \"AM_A\",\"END_A\", \"IE_B\", \"BP_A\",\"ARR_B\", \"VR_B\", \"ARR_A\",\"t\",\"crystal_system\"]\n",
    "df = df[features]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHTQxS7Wz-TP",
    "outputId": "f5f095a8-6200-4c3a-94c5-fbb7b495168c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è± Run 1 of 5...\n",
      "\n",
      "üîß Tuning Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8724, F1: 0.8700, Precision: 0.8728, Recall: 0.8695, ROC AUC: 0.9817, ‚è± 878.51 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8759, F1: 0.8738, Precision: 0.8765, Recall: 0.8733, ROC AUC: 0.9776, ‚è± 47.79 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2457\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8740, F1: 0.8712, Precision: 0.8742, Recall: 0.8710, ROC AUC: 0.9830, ‚è± 71.44 sec\n",
      "\n",
      "‚è± Run 2 of 5...\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8637, F1: 0.8614, Precision: 0.8637, Recall: 0.8608, ROC AUC: 0.9813, ‚è± 86.89 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8742, F1: 0.8725, Precision: 0.8750, Recall: 0.8717, ROC AUC: 0.9776, ‚è± 8.90 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2455\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8666, F1: 0.8645, Precision: 0.8676, Recall: 0.8640, ROC AUC: 0.9822, ‚è± 5.75 sec\n",
      "\n",
      "‚è± Run 3 of 5...\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8705, F1: 0.8673, Precision: 0.8690, Recall: 0.8675, ROC AUC: 0.9803, ‚è± 87.51 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8763, F1: 0.8735, Precision: 0.8750, Recall: 0.8737, ROC AUC: 0.9776, ‚è± 9.01 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2453\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8740, F1: 0.8710, Precision: 0.8733, Recall: 0.8714, ROC AUC: 0.9828, ‚è± 5.83 sec\n",
      "\n",
      "‚è± Run 4 of 5...\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8736, F1: 0.8711, Precision: 0.8736, Recall: 0.8706, ROC AUC: 0.9808, ‚è± 91.85 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8746, F1: 0.8725, Precision: 0.8749, Recall: 0.8721, ROC AUC: 0.9767, ‚è± 12.30 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2454\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8703, F1: 0.8675, Precision: 0.8703, Recall: 0.8675, ROC AUC: 0.9835, ‚è± 7.58 sec\n",
      "\n",
      "‚è± Run 5 of 5...\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "Random Forest Accuracy: 0.8734, F1: 0.8706, Precision: 0.8726, Recall: 0.8702, ROC AUC: 0.9817, ‚è± 95.03 sec\n",
      "\n",
      "üîß Tuning Extra Trees...\n",
      "Extra Trees Accuracy: 0.8755, F1: 0.8731, Precision: 0.8752, Recall: 0.8727, ROC AUC: 0.9769, ‚è± 10.47 sec\n",
      "\n",
      "üîß Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2451\n",
      "[LightGBM] [Info] Number of data points in the train set: 19333, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -2.074746\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.908547\n",
      "[LightGBM] [Info] Start training from score -2.043924\n",
      "[LightGBM] [Info] Start training from score -1.896413\n",
      "[LightGBM] [Info] Start training from score -1.888861\n",
      "[LightGBM] [Info] Start training from score -1.930053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.8680, F1: 0.8654, Precision: 0.8676, Recall: 0.8650, ROC AUC: 0.9840, ‚è± 6.28 sec\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Classification: RF, ET, LGBM only ----------------\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def tune_classifiers(X_train, y_train, X_test, y_test, cv=3, best_params_dict=None):\n",
    "    results = {}\n",
    "\n",
    "    classifiers = {\n",
    "        \"Random Forest\": (RandomForestClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [500, 1000],\n",
    "            'clf__max_depth': [50, None],\n",
    "            'clf__max_features': ['sqrt', 'log2']\n",
    "        }),\n",
    "        \"Extra Trees\": (ExtraTreesClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 300],\n",
    "            'clf__max_depth': [None, 50]\n",
    "        }),\n",
    "        \"LightGBM\": (LGBMClassifier(random_state=242), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.05, 0.1],\n",
    "            'clf__num_leaves': [15, 31]\n",
    "        })\n",
    "    }\n",
    "\n",
    "    for name, (model, param_grid) in classifiers.items():\n",
    "        print(f\"\\nüîß Tuning {name}...\")\n",
    "        start = time.time()\n",
    "\n",
    "        if best_params_dict and name in best_params_dict:\n",
    "            # Reuse best params\n",
    "            model_params = {k.replace('clf__', ''): v for k, v in best_params_dict[name].items()}\n",
    "            model.set_params(**model_params)\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            best_model = pipe\n",
    "            best_params = best_params_dict[name]\n",
    "        else:\n",
    "            # Tune with GridSearch\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n",
    "            search = GridSearchCV(pipe, param_grid=param_grid, cv=cv, n_jobs=-1, scoring='accuracy', verbose=0)\n",
    "            search.fit(X_train, y_train)\n",
    "            best_model = search.best_estimator_\n",
    "            best_params = search.best_params_\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_prob = best_model.predict_proba(X_test) if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        roc = roc_auc_score(y_test, y_prob, multi_class='ovr') if y_prob is not None else np.nan\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(f\"{name} Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, \"\n",
    "              f\"Recall: {recall:.4f}, ROC AUC: {roc:.4f}, ‚è± {elapsed:.2f} sec\")\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': acc,\n",
    "            'F1 score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'ROC AUC': roc,\n",
    "            'best_model': best_model,\n",
    "            'best_params': best_params,\n",
    "            'time': elapsed\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"crystal_system\"])\n",
    "y = df[\"crystal_system\"]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---- Repeated Runs ----\n",
    "runs = 5\n",
    "aggregated_results = defaultdict(list)\n",
    "best_param_store = {}\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"\\n‚è± Run {i+1} of {runs}...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=242 + i, stratify=y\n",
    "    )\n",
    "\n",
    "    if i == 0:\n",
    "        run_results = tune_classifiers(X_train, y_train, X_test, y_test, cv=3)\n",
    "        best_param_store = {model_name: result['best_params'] for model_name, result in run_results.items()}\n",
    "    else:\n",
    "        run_results = tune_classifiers(X_train, y_train, X_test, y_test, cv=3, best_params_dict=best_param_store)\n",
    "\n",
    "    for model_name, scores in run_results.items():\n",
    "        aggregated_results[model_name].append(scores)\n",
    "\n",
    "# ---- Averaging results ----\n",
    "summary = {}\n",
    "for model, runs_scores in aggregated_results.items():\n",
    "    summary[model] = {\n",
    "        'Avg Accuracy': round(np.mean([r['accuracy'] for r in runs_scores]), 4),\n",
    "        'Avg F1 Score': round(np.mean([r['F1 score'] for r in runs_scores]), 4),\n",
    "        'Avg Precision': round(np.mean([r['precision'] for r in runs_scores]), 4),\n",
    "        'Avg Recall': round(np.mean([r['recall'] for r in runs_scores]), 4),\n",
    "        'Avg ROC AUC': round(np.nanmean([r['ROC AUC'] for r in runs_scores]), 4),\n",
    "        'Avg Time (sec)': round(np.mean([r['time'] for r in runs_scores]), 2),\n",
    "        'Best Params (from first run)': best_param_store[model]\n",
    "    }\n",
    "\n",
    "summary_df = pd.DataFrame(summary).T.reset_index().rename(columns={'index': 'Model'})\n",
    "summary_df.to_csv(\"M7_reduced_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izkrTCB597Y4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
